/// Video-AI Pipeline Integration - Connects video decoder to YOLOv5n object detection
package adas:video-ai-pipeline@0.1.0;

world video-ai-pipeline-component {
    /// Import video decoder for frame data
    import video-decoder: interface {
        /// Video control interface
        record video-config {
            auto-play: bool,
            loop-enabled: bool,
            playback-speed: f32,
            start-frame: u32,
        }

        enum video-status {
            unloaded,
            loading,
            ready,
            playing,
            paused,
            ended,
            error,
        }

        record video-info {
            width: u32,
            height: u32,
            frame-count: u32,
            frame-rate: f32,
            duration-ms: u64,
            file-size: u64,
        }

        /// Camera data interface
        record position3d {
            x: f64,
            y: f64,
            z: f64,
        }

        record quaternion {
            x: f64,
            y: f64,
            z: f64,
            w: f64,
        }

        record camera-frame {
            width: u32,
            height: u32,
            data: list<u8>,
            format: pixel-format,
            timestamp: u64,
            exposure-time: f32,
            gain: f32,
            sensor-pose: camera-pose,
        }

        record camera-pose {
            position: position3d,
            orientation: quaternion,
        }

        enum pixel-format {
            rgb8,
            rgba8,
            bgr8,
            bgra8,
            yuv420,
            gray8,
            gray16,
        }

        record camera-intrinsics {
            focal-length-x: f64,
            focal-length-y: f64,
            principal-point-x: f64,
            principal-point-y: f64,
            distortion: list<f64>,
        }

        resource camera-stream {
            get-frame: func() -> result<camera-frame, string>;
            get-intrinsics: func() -> camera-intrinsics;
            is-available: func() -> bool;
        }
        
        create-stream: func() -> camera-stream;
        load-embedded-video: func() -> result<video-info, string>;
        play: func() -> result<_, string>;
        pause: func() -> result<_, string>;
        get-status: func() -> video-status;
    }

    /// Import object detection AI
    import object-detection: interface {
        /// Detection results
        record position2d {
            x: f64,
            y: f64,
        }

        record bounding-box {
            x: f64,
            y: f64,
            width: f64,
            height: f64,
        }

        enum object-class {
            vehicle,
            pedestrian,
            cyclist,
            traffic-sign,
            traffic-light,
            road-marking,
            obstacle,
            animal,
            unknown,
        }

        record detected-object {
            class: object-class,
            confidence: f32,
            bounding-box: bounding-box,
            center: position2d,
            velocity: option<position2d>,
            track-id: option<u32>,
        }

        record detection-metrics {
            total-objects: u32,
            processing-time-ms: f64,
            confidence-average: f32,
            frame-rate: f32,
        }

        resource detection-stream {
            get-detections: func() -> result<list<detected-object>, string>;
            get-metrics: func() -> detection-metrics;
            is-available: func() -> bool;
        }

        create-stream: func() -> detection-stream;
        process-frame: func(frame-data: list<u8>, width: u32, height: u32) -> result<list<detected-object>, string>;
    }

    /// Export pipeline control interface
    export pipeline-control: interface {
        /// Pipeline configuration
        record pipeline-config {
            enable-video-playback: bool,
            enable-object-detection: bool,
            detection-confidence-threshold: f32,
            max-fps: f32,
        }

        enum pipeline-status {
            stopped,
            initializing,
            running,
            paused,
            error,
        }

        record pipeline-metrics {
            frames-processed: u32,
            detections-count: u32,
            avg-processing-time-ms: f64,
            current-fps: f32,
        }

        /// Pipeline control
        start-pipeline: func(config: pipeline-config) -> result<_, string>;
        stop-pipeline: func() -> result<_, string>;
        pause-pipeline: func() -> result<_, string>;
        resume-pipeline: func() -> result<_, string>;
        
        /// Status and metrics
        get-status: func() -> pipeline-status;
        get-metrics: func() -> pipeline-metrics;
        
        /// Process single frame (for testing)
        process-single-frame: func() -> result<string, string>;
        
        /// Run diagnostics
        run-diagnostics: func() -> result<string, string>;
    }
}