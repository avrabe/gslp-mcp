package adas:sensor-fusion@0.1.0;

/// Multi-modal sensor fusion ECU using Extended Kalman Filter and AI
interface sensor-fusion {
    /// Fused object representation
    record fused-object {
        object-id: u32,
        position: position3d,
        velocity: velocity3d,
        acceleration: acceleration3d,
        dimensions: dimensions3d,
        object-type: object-type,
        confidence: f32,
        covariance-matrix: list<f64>,   // 9x9 state covariance
        sensor-sources: list<sensor-source>,
        track-age: u32,
        prediction-horizon: f32,        // seconds
    }

    record position3d {
        x: f64,
        y: f64,
        z: f64,
    }

    record velocity3d {
        vx: f32,
        vy: f32,
        vz: f32,
    }

    record acceleration3d {
        ax: f32,
        ay: f32,
        az: f32,
    }

    record dimensions3d {
        length: f32,
        width: f32,
        height: f32,
    }

    enum object-type {
        vehicle,
        truck,
        motorcycle,
        pedestrian,
        cyclist,
        animal,
        stationary-object,
        unknown,
    }

    enum sensor-source {
        front-camera,
        surround-camera,
        front-radar,
        corner-radar,
        lidar,
        ultrasonic,
    }

    /// Sensor data input for fusion
    record sensor-input {
        sensor-id: string,
        sensor-type: sensor-source,
        timestamp: u64,
        data: sensor-data,
        quality-metrics: data-quality,
    }

    variant sensor-data {
        camera-detection(camera-detection),
        radar-target(radar-target),
        lidar-object(lidar-object),
        ultrasonic-reading(ultrasonic-reading),
    }

    record camera-detection {
        object-id: u32,
        bounding-box: bounding-box,
        class-confidence: f32,
        depth-estimate: option<f32>,
    }

    record radar-target {
        range: f32,
        azimuth: f32,
        elevation: f32,
        range-rate: f32,
        rcs: f32,                   // radar cross section
    }

    record lidar-object {
        centroid: position3d,
        point-count: u32,
        bounding-box: bounding-box3d,
    }

    record ultrasonic-reading {
        distance: f32,
        beam-angle: f32,
        sensor-position: position3d,
    }

    record bounding-box {
        x: f32,
        y: f32,
        width: f32,
        height: f32,
    }

    record bounding-box3d {
        min-x: f64,
        min-y: f64,
        min-z: f64,
        max-x: f64,
        max-y: f64,
        max-z: f64,
    }

    record data-quality {
        signal-to-noise-ratio: f32,
        confidence: f32,
        timestamp-accuracy: f32,
        calibration-status: bool,
    }

    /// Fusion algorithm configuration
    record fusion-config {
        algorithm-type: fusion-algorithm,
        temporal-window: f32,       // seconds
        spatial-threshold: f32,     // meters
        confidence-threshold: f32,
        max-track-age: u32,         // frames
        prediction-model: motion-model,
    }

    enum fusion-algorithm {
        extended-kalman-filter,
        unscented-kalman-filter,
        particle-filter,
        neural-fusion,
    }

    enum motion-model {
        constant-velocity,
        constant-acceleration,
        coordinated-turn,
        bicycle-model,
    }

    /// Environmental scene understanding
    record scene-state {
        ego-vehicle-state: vehicle-state,
        detected-objects: list<fused-object>,
        free-space: free-space-map,
        road-geometry: road-info,
        weather-conditions: weather-state,
        scene-confidence: f32,
    }

    record vehicle-state {
        position: position3d,
        velocity: velocity3d,
        acceleration: acceleration3d,
        heading: f32,               // radians
        steering-angle: f32,        // radians
        yaw-rate: f32,             // rad/s
    }

    record free-space-map {
        grid-resolution: f32,       // meters per cell
        grid-size: grid-dimensions,
        occupancy-grid: list<u8>,   // 0-255 occupancy probability
        confidence-grid: list<u8>,
    }

    record grid-dimensions {
        width: u32,
        height: u32,
        origin-x: f32,
        origin-y: f32,
    }

    record road-info {
        lane-markings: list<lane-marking>,
        road-boundaries: list<boundary>,
        traffic-signs: list<traffic-sign>,
        road-curvature: f32,
        lane-width: f32,
    }

    record lane-marking {
        points: list<position3d>,
        marking-type: lane-type,
        confidence: f32,
    }

    enum lane-type {
        solid,
        dashed,
        double,
        merging,
    }

    record boundary {
        points: list<position3d>,
        boundary-type: boundary-type,
    }

    enum boundary-type {
        curb,
        barrier,
        fence,
        vegetation,
    }

    record traffic-sign {
        position: position3d,
        sign-type: string,
        confidence: f32,
    }

    record weather-state {
        visibility: f32,            // meters
        precipitation: precipitation-type,
        temperature: f32,           // Celsius
        wind-speed: f32,           // m/s
    }

    enum precipitation-type {
        none,
        light-rain,
        heavy-rain,
        snow,
        hail,
    }

    /// Initialize fusion system
    initialize: func(config: fusion-config) -> result<_, string>;

    /// Add sensor input for fusion
    add-sensor-input: func(input: sensor-input) -> result<_, string>;

    /// Perform sensor fusion and get scene state
    fuse-sensors: func() -> result<scene-state, string>;

    /// Get tracked objects
    get-tracked-objects: func() -> result<list<fused-object>, string>;

    /// Predict object states at future time
    predict-future-state: func(time-horizon: f32) -> result<list<fused-object>, string>;

    /// Update fusion configuration
    update-config: func(config: fusion-config) -> result<_, string>;

    /// Get fusion performance metrics
    get-performance: func() -> result<fusion-metrics, string>;

    record fusion-metrics {
        processing-latency: f32,    // milliseconds
        track-accuracy: f32,
        false-positive-rate: f32,
        missed-detection-rate: f32,
        temporal-consistency: f32,
        memory-usage: u32,          // MB
    }

    /// Reset all tracks
    reset-tracks: func() -> result<_, string>;

    /// Get debug information
    get-debug-info: func() -> result<debug-info, string>;

    record debug-info {
        active-tracks: u32,
        sensor-input-rates: list<sensor-rate>,
        fusion-statistics: fusion-stats,
    }

    record sensor-rate {
        sensor-id: string,
        update-rate: f32,           // Hz
        data-quality: f32,
    }

    record fusion-stats {
        successful-associations: u32,
        failed-associations: u32,
        new-track-initiations: u32,
        track-terminations: u32,
    }
}

world sensor-fusion-component {
    export sensor-fusion;
}