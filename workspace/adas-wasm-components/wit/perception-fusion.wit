/// Perception Fusion Component - IMPORTS sensor-fusion and AI data, EXPORTS perception model
package adas:perception-fusion@0.1.0;

world perception-fusion-component {
    /// Import fused environment data from sensor fusion
    import fusion-data: interface {
        record position3d {
            x: f64,
            y: f64,
            z: f64,
        }

        record velocity3d {
            vx: f64,
            vy: f64,
            vz: f64,
            speed: f64,
        }

        record environment-model {
            objects: list<fused-object>,
            timestamp: u64,
            fusion-quality: f32,
            coverage-area: coverage-area,
        }

        record fused-object {
            object-id: u32,
            object-type: object-type,
            position: position3d,
            velocity: velocity3d,
            confidence: f32,
            source-sensors: list<sensor-type>,
            tracking-state: tracking-state,
        }

        record coverage-area {
            forward-range: f64,
            lateral-range: f64,
            angular-coverage: f32,
        }

        enum object-type {
            unknown,
            vehicle,
            pedestrian,
            cyclist,
            motorcycle,
            truck,
            bus,
            traffic-sign,
            traffic-light,
            construction-zone,
            road-debris,
        }

        enum sensor-type {
            camera,
            radar,
            lidar,
            ultrasonic,
        }

        enum tracking-state {
            new,
            tracked,
            lost,
            deleted,
        }

        resource fusion-stream {
            get-environment: func() -> result<environment-model, string>;
            is-available: func() -> bool;
            get-object-count: func() -> u32;
        }
        
        create-stream: func() -> fusion-stream;
    }

    /// Import behavior predictions from AI
    import prediction-data: interface {
        record position3d {
            x: f64,
            y: f64,
            z: f64,
        }

        record velocity3d {
            vx: f64,
            vy: f64,
            vz: f64,
            speed: f64,
        }

        record acceleration3d {
            ax: f64,
            ay: f64,
            az: f64,
            magnitude: f64,
        }

        record prediction-results {
            predictions: list<behavior-prediction>,
            timestamp: u64,
            prediction-horizon: f32,
            confidence-level: f32,
        }

        record behavior-prediction {
            object-id: u32,
            predicted-behavior: behavior-type,
            trajectory: predicted-trajectory,
            intention: intention-type,
            risk-level: risk-level,
            confidence: f32,
        }

        record predicted-trajectory {
            waypoints: list<trajectory-point>,
            duration: f32,
            probability: f32,
        }

        record trajectory-point {
            position: position3d,
            velocity: velocity3d,
            acceleration: acceleration3d,
            timestamp: f32,
        }

        enum behavior-type {
            straight-motion,
            lane-change-left,
            lane-change-right,
            turning-left,
            turning-right,
            stopping,
            accelerating,
            decelerating,
            merging,
            yielding,
            unknown,
        }

        enum intention-type {
            continue-straight,
            change-lane,
            turn,
            stop,
            merge,
            overtake,
            park,
            u-turn,
            unknown,
        }

        enum risk-level {
            none,
            low,
            medium,
            high,
            critical,
        }

        resource prediction-stream {
            get-predictions: func() -> result<prediction-results, string>;
            is-available: func() -> bool;
            get-prediction-count: func() -> u32;
        }
        
        create-stream: func() -> prediction-stream;
    }
    
    /// Export unified perception model
    export perception-data: interface {
        record position3d {
            x: f64,
            y: f64,
            z: f64,
        }

        record velocity3d {
            vx: f64,
            vy: f64,
            vz: f64,
            speed: f64,
        }

        record acceleration3d {
            ax: f64,
            ay: f64,
            az: f64,
            magnitude: f64,
        }

        record perception-model {
            perceived-objects: list<perceived-object>,
            scene-understanding: scene-context,
            risk-assessment: risk-assessment,
            timestamp: u64,
            confidence: f32,
        }

        record perceived-object {
            object-id: u32,
            object-type: object-type,
            position: position3d,
            velocity: velocity3d,
            predicted-trajectory: predicted-trajectory,
            semantic-attributes: semantic-attributes,
            risk-level: risk-level,
            confidence: f32,
        }

        record predicted-trajectory {
            waypoints: list<trajectory-point>,
            duration: f32,
            probability: f32,
        }

        record trajectory-point {
            position: position3d,
            velocity: velocity3d,
            acceleration: acceleration3d,
            timestamp: f32,
        }

        record semantic-attributes {
            size-category: size-category,
            movement-state: movement-state,
            interaction-potential: f32,
            lane-association: lane-association,
        }

        record scene-context {
            traffic-density: traffic-density,
            weather-conditions: weather-conditions,
            lighting-conditions: lighting-conditions,
            road-type: road-type,
            intersection-nearby: bool,
        }

        record risk-assessment {
            overall-risk: risk-level,
            collision-probability: f32,
            time-to-collision: f32,
            critical-objects: list<u32>,
            recommended-actions: list<action-recommendation>,
        }

        enum object-type {
            unknown,
            vehicle,
            pedestrian,
            cyclist,
            motorcycle,
            truck,
            bus,
            traffic-sign,
            traffic-light,
            construction-zone,
            road-debris,
        }

        enum size-category {
            small,
            medium,
            large,
            oversized,
        }

        enum movement-state {
            stationary,
            slow-moving,
            normal-speed,
            fast-moving,
            erratic,
        }

        enum lane-association {
            same-lane,
            adjacent-left,
            adjacent-right,
            oncoming,
            crossing,
            off-road,
        }

        enum traffic-density {
            light,
            moderate,
            heavy,
            congested,
        }

        enum weather-conditions {
            clear,
            cloudy,
            rain,
            snow,
            fog,
            extreme,
        }

        enum lighting-conditions {
            daylight,
            dusk,
            night,
            artificial,
        }

        enum road-type {
            city-street,
            highway,
            rural-road,
            parking-lot,
            construction-zone,
        }

        enum risk-level {
            none,
            low,
            medium,
            high,
            critical,
        }

        enum action-recommendation {
            none,
            monitor,
            prepare-brake,
            brake,
            emergency-stop,
            steer-left,
            steer-right,
            slow-down,
            change-lane,
        }

        resource perception-stream {
            get-perception: func() -> result<perception-model, string>;
            is-available: func() -> bool;
            get-object-count: func() -> u32;
        }
        
        create-stream: func() -> perception-stream;
    }
    
    /// Export perception control interface
    export perception-control: interface {
        record perception-config {
            fusion-weight: f32,
            prediction-weight: f32,
            risk-sensitivity: f32,
            update-frequency: u32,
            scene-analysis-enabled: bool,
            risk-assessment-enabled: bool,
        }

        enum perception-status {
            offline,
            initializing,
            processing,
            degraded,
            error,
        }

        record performance-metrics {
            processing-time-ms: f32,
            fusion-accuracy: f32,
            prediction-accuracy: f32,
            risk-detection-rate: f32,
            cpu-usage-percent: f32,
            memory-usage-mb: u32,
        }

        record diagnostic-result {
            fusion-health: test-result,
            prediction-health: test-result,
            scene-analysis: test-result,
            risk-assessment: test-result,
            overall-score: f32,
        }

        enum test-result {
            passed,
            failed,
            warning,
            not-tested,
        }

        /// Initialize perception system
        initialize: func(config: perception-config) -> result<_, string>;
        
        /// Start perception processing
        start-perception: func() -> result<_, string>;
        
        /// Stop perception processing  
        stop-perception: func() -> result<_, string>;
        
        /// Update configuration
        update-config: func(config: perception-config) -> result<_, string>;
        
        /// Get system status
        get-status: func() -> perception-status;
        
        /// Get performance metrics
        get-performance: func() -> performance-metrics;
        
        /// Run diagnostics
        run-diagnostic: func() -> result<diagnostic-result, string>;
    }
}